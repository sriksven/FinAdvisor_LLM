{"cells":[{"cell_type":"markdown","metadata":{"id":"FOxMV6FVYSV6"},"source":["# Inference notebook\n"]},{"cell_type":"markdown","metadata":{"id":"DUTjDjNzYgwQ"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PS_TLYd3ZhWm"},"outputs":[],"source":["!pip install datasets\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install accelerate\n","!pip install bitsandbytes\n","!pip install git+https://github.com/huggingface/peft.git@main"]},{"cell_type":"markdown","metadata":{"id":"P7FN0j42Yi0-"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQKDzk2rOn38"},"outputs":[],"source":["from peft import PeftModel\n","from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n","\n","tokenizer = LlamaTokenizer.from_pretrained(\"baffo32/decapoda-research-llama-7B-hf\")\n","\n","model = LlamaForCausalLM.from_pretrained(\n","    \"baffo32/decapoda-research-llama-7B-hf\",\n","    load_in_8bit=True,\n","    device_map=\"auto\",\n",")\n","model = PeftModel.from_pretrained(model, \"kunchum/llama-7b-fine-tuned\")\n","\n","# PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","# ### Instruction:\n","# Tell me something about alpacas.\n","# ### Response:\"\"\"\n","\n","# inputs = tokenizer(\n","#     PROMPT,\n","#     return_tensors=\"pt\",\n","# )\n","# input_ids = inputs[\"input_ids\"].cuda()\n","\n","# generation_config = GenerationConfig(\n","#     temperature=0.6,\n","#     top_p=0.95,\n","#     repetition_penalty=1.15,\n","# )\n","# print(\"Generating...\")\n","# generation_output = model.generate(\n","#     input_ids=input_ids,\n","#     generation_config=generation_config,\n","#     return_dict_in_generate=True,\n","#     output_scores=True,\n","#     max_new_tokens=128,\n","# )\n","# for s in generation_output.sequences:\n","#     print(tokenizer.decode(s))"]},{"cell_type":"markdown","metadata":{"id":"bWjd_0VrY-7O"},"source":["### Inference"]},{"cell_type":"markdown","metadata":{"id":"sYsAnKWxZBEt"},"source":["Enter the input text into the variable `custom_prompt`"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":230,"status":"ok","timestamp":1730174865300,"user":{"displayName":"Rakshak Kunchum","userId":"10315493721906858371"},"user_tz":240},"id":"leSHioluYzQW"},"outputs":[],"source":["custom_prompt = \"When should I get a second credit card?\""]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":220,"status":"ok","timestamp":1730174866813,"user":{"displayName":"Rakshak Kunchum","userId":"10315493721906858371"},"user_tz":240},"id":"h9srLgUJPx0H"},"outputs":[],"source":["PROMPT =f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","### Instruction:\n","{custom_prompt}\n","### Response:\n","'''"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16834,"status":"ok","timestamp":1730175028146,"user":{"displayName":"Rakshak Kunchum","userId":"10315493721906858371"},"user_tz":240},"id":"WymHNdeuSJTF","outputId":"b3f7c42e-d71c-4feb-defa-81ca959489f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Generating...\n","\n","\n","\n"," ⁇  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","### Instruction:\n","When should I get a second credit card?\n","### Response:\n","You can apply for your first credit card when you are 18 years old and have steady income, such as from employment or investments. You may also be able to qualify if you're married or live with someone who has good credit history. If you don’t meet these requirements yet, it might make sense to wait until you do before applying for another credit card.\n","CPU times: user 16.5 s, sys: 50.3 ms, total: 16.6 s\n","Wall time: 16.6 s\n"]}],"source":["%%time\n","\n","inputs = tokenizer(\n","    PROMPT,\n","    return_tensors=\"pt\",\n",")\n","input_ids = inputs[\"input_ids\"].cuda()\n","\n","generation_config = GenerationConfig(\n","    temperature=0.6,\n","    top_p=0.95,\n","    repetition_penalty=1.15,\n",")\n","print(\"\\n\\n\\nGenerating Response...\\n\\n\\n\")\n","generation_output = model.generate(\n","    input_ids=input_ids,\n","    generation_config=generation_config,\n","    return_dict_in_generate=True,\n","    output_scores=True,\n","    max_new_tokens=256,\n",")\n","for s in generation_output.sequences:\n","    print(tokenizer.decode(s))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSF6lCSASPHg"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1lIlJpsMq4JP1GszUw_oZhyKr2irsMcq_","timestamp":1730154391394}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
