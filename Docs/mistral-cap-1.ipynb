{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["%pip install evaluate"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:28:53.052764Z","iopub.execute_input":"2024-11-22T00:28:53.053064Z","iopub.status.idle":"2024-11-22T00:29:03.098517Z","shell.execute_reply.started":"2024-11-22T00:28:53.053039Z","shell.execute_reply":"2024-11-22T00:29:03.097517Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Gkk90g-Jajjd","executionInfo":{"status":"ok","timestamp":1732237774734,"user_tz":300,"elapsed":21544,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}},"outputId":"999007ae-98ec-42a6-a2a7-5aac7b70cdee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Collecting datasets>=2.0.0 (from evaluate)\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Collecting dill (from evaluate)\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n","Collecting xxhash (from evaluate)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n","Collecting dill (from evaluate)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"execution_count":1},{"cell_type":"code","source":["%pip install bitsandbytes"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:03.100136Z","iopub.execute_input":"2024-11-22T00:29:03.100428Z","iopub.status.idle":"2024-11-22T00:29:15.934846Z","shell.execute_reply.started":"2024-11-22T00:29:03.100398Z","shell.execute_reply":"2024-11-22T00:29:15.933784Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"wtdpfVvvajjg","executionInfo":{"status":"ok","timestamp":1732237798864,"user_tz":300,"elapsed":24136,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}},"outputId":"1e0961c6-59ae-4d18-e1be-8123eeddc185"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.44.1\n"]}],"execution_count":2},{"cell_type":"code","source":["%pip install peft"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:15.936082Z","iopub.execute_input":"2024-11-22T00:29:15.936380Z","iopub.status.idle":"2024-11-22T00:29:24.951246Z","shell.execute_reply.started":"2024-11-22T00:29:15.936349Z","shell.execute_reply":"2024-11-22T00:29:24.950123Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"b7CZgsMlajjh","executionInfo":{"status":"ok","timestamp":1732237807365,"user_tz":300,"elapsed":8512,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}},"outputId":"0f4f4599-2761-4c1b-f252-dc6cba7bf66a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n"]}],"execution_count":3},{"cell_type":"code","source":["import os\n","import sys\n","import json\n","import evaluate\n","import math\n","import torch\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","from transformers import BitsAndBytesConfig\n","\n","from datasets import load_dataset\n","import transformers\n","\n","# Assuming MISTRAL has equivalent classes, replace LlamaForCausalLM and LlamaTokenizer\n","from transformers import AutoTokenizer\n","\n","from peft import (\n","    prepare_model_for_kbit_training,\n","    LoraConfig,\n","    get_peft_model,\n","    get_peft_model_state_dict,\n",")\n","\n","from kaggle_secrets import UserSecretsClient\n","import wandb"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:24.953878Z","iopub.execute_input":"2024-11-22T00:29:24.954598Z","iopub.status.idle":"2024-11-22T00:29:42.287058Z","shell.execute_reply.started":"2024-11-22T00:29:24.954557Z","shell.execute_reply":"2024-11-22T00:29:42.286366Z"},"colab":{"base_uri":"https://localhost:8080/","height":405},"id":"yhT56mv0ajjh","executionInfo":{"status":"error","timestamp":1732237857697,"user_tz":300,"elapsed":50343,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}},"outputId":"b19c9d7f-a2b9-47d5-80ca-ef9c77f7b442"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'kaggle_secrets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-c4bf8432e857>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"execution_count":4},{"cell_type":"code","source":["# wandb login\n","user_secrets = UserSecretsClient()\n","wandb_key = user_secrets.get_secret('wandb-key')\n","wandb.login(key=wandb_key)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:42.288142Z","iopub.execute_input":"2024-11-22T00:29:42.288479Z","iopub.status.idle":"2024-11-22T00:29:43.236417Z","shell.execute_reply.started":"2024-11-22T00:29:42.288443Z","shell.execute_reply":"2024-11-22T00:29:43.235549Z"},"id":"dR2qxF-qajjh","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":16,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Set random seed for reproducibility\n","RANDOM_SEED = 1234\n","transformers.set_seed(RANDOM_SEED)\n","\n","# Training configuration\n","MICRO_BATCH_SIZE = 4\n","BATCH_SIZE = 128\n","GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n","EPOCHS = 1  # Adjust based on your computational limits and model size\n","LEARNING_RATE = 2e-5  # Standard fine-tuning learning rate\n","CUTOFF_LEN = 256  # Adjust if MISTRAL processes data differently\n","LORA_R = 8  # LoRA parameters, only adjust if using LoRA with MISTRAL\n","LORA_ALPHA = 16\n","LORA_DROPOUT = 0.05\n","VAL_SET_SIZE = 0  # Set to a positive number if validation is needed\n","TARGET_MODULES = [\n","    'q_proj',\n","    'v_prol',\n","]  # Specific to model internals, check if applicable for MISTRAL\n","OUTPUT_DIR = '/kaggle/working/mistral_model_tuned'  # Update path for MISTRAL outputs\n","\n","# DDP (Distributed Data Parallel) settings\n","device_map = 'auto'\n","world_size = int(os.environ.get('WORLD_SIZE', 1))\n","ddp = (world_size != 1)  # Enable DDP if more than one GPU is used\n","if ddp:\n","    device_map = {'': int(os.environ.get('LOCAL_RANK') or 0)}\n","    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:48.031028Z","iopub.execute_input":"2024-11-22T00:29:48.032119Z","iopub.status.idle":"2024-11-22T00:29:48.045944Z","shell.execute_reply.started":"2024-11-22T00:29:48.032082Z","shell.execute_reply":"2024-11-22T00:29:48.045154Z"},"id":"LmRrPRstajji","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":15,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=\"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\")\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:51.319268Z","iopub.execute_input":"2024-11-22T00:29:51.320145Z","iopub.status.idle":"2024-11-22T00:29:51.434298Z","shell.execute_reply.started":"2024-11-22T00:29:51.320106Z","shell.execute_reply":"2024-11-22T00:29:51.433458Z"},"id":"gtgil0liajji","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":15,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n","model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:29:52.915105Z","iopub.execute_input":"2024-11-22T00:29:52.915456Z","iopub.status.idle":"2024-11-22T00:38:06.414720Z","shell.execute_reply.started":"2024-11-22T00:29:52.915423Z","shell.execute_reply":"2024-11-22T00:38:06.412955Z"},"id":"adkA5GP5ajji","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":14,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model  # Assuming these are imported correctly\n","\n","# Define the model checkpoint\n","model_checkpoint = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","# Optional: Configure quantization to save VRAM\n","quantization_config = BitsAndBytesConfig(\n","    load_in_8bit=True,  # This assumes quantization config is applicable\n","    # Uncomment and adjust if the model supports advanced quantization features:\n","    # bnb_4bit_compute_dtype=torch.bfloat16,\n","    # bnb_4bit_use_double_quant=True,\n",")\n","\n","# Load the model from the checkpoint with quantization settings if supported\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_checkpoint,\n","    quantization_config=quantization_config,  # Apply quantization config if supported\n","    device_map='auto'  # Automatically map the model to the available GPU(s)\n",")\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","# LoRA configuration - adjust parameters as needed\n","lora_config = LoraConfig(\n","    r=8,  # Rank of the LoRA layers\n","    lora_alpha=16,  # Scaling factor for LoRA\n","    target_modules=['q_proj', 'v_proj'],  # Typically targets Q, K, V projections in transformers\n","    lora_dropout=0.05,  # Dropout rate for LoRA layers\n","    bias='none',  # No bias term in the LoRA layers\n","    task_type='CAUSAL_LM',  # Specify the task type, ensure this aligns with your model type\n",")\n","\n","# Apply the LoRA configuration to the model\n","model = get_peft_model(model, lora_config)\n","\n","# Example usage of the model to generate text (you can customize further as needed)\n","from transformers import pipeline\n","generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","prompt = \"Discuss the future implications of AI in healthcare.\"\n","outputs = generator(prompt, max_length=100, num_return_sequences=1)\n","\n","# Print the generated text\n","for output in outputs:\n","    print(output['generated_text'])\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:38:09.331265Z","iopub.execute_input":"2024-11-22T00:38:09.331656Z","iopub.status.idle":"2024-11-22T00:39:43.224875Z","shell.execute_reply.started":"2024-11-22T00:38:09.331624Z","shell.execute_reply":"2024-11-22T00:39:43.223370Z"},"id":"yiwL3zFxajjj","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":14,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict\n","\n","# Load your dataset from the Hugging Face Hub\n","data = load_dataset('kunchum/capstone_1')\n","\n","# Shuffle the dataset using a predefined seed for reproducibility\n","RANDOM_SEED = 1234\n","data = data.shuffle(seed=RANDOM_SEED)\n","\n","# Select a sample of 20,000 records for fine-tuning\n","sample_size = 20000\n","data_sample = data['train'].select(range(sample_size))\n","\n","# Create a DatasetDict with the sampled data to maintain a structured format\n","sampled_data_dict = DatasetDict({\n","    'train': data_sample  # Optionally, add more splits like 'validation' or 'test' if necessary\n","})\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:40:39.615537Z","iopub.execute_input":"2024-11-22T00:40:39.616006Z","iopub.status.idle":"2024-11-22T00:40:43.148133Z","shell.execute_reply.started":"2024-11-22T00:40:39.615953Z","shell.execute_reply":"2024-11-22T00:40:43.147207Z"},"id":"rkGZT73Xajjj","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":13,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def generate_prompt(data_point):\n","    \"\"\"\n","    Generate input text based on a prompt, task instruction, context information (if available),\n","    and a response. The function creates a formatted string that includes the task instruction,\n","    optional context, and the expected response.\n","\n","    :param data_point: Dictionary containing 'instruction', optional 'context_cleaned', and 'response_cleaned'\n","    :return: Formatted string to be used as input text\n","    \"\"\"\n","    # Define the base prompt structure with mandatory instruction and response sections\n","    prompt = \"Below is an instruction that describes a task.\"\n","    prompt += \"\\n\\n### Instruction:\\n\" + data_point[\"instruction\"]\n","\n","    # Add context information if available\n","    if data_point.get('context_cleaned'):\n","        prompt += \"\\n\\n### Input:\\n\" + data_point[\"context_cleaned\"]\n","\n","    # Append the response part of the prompt\n","    prompt += \"\\n\\n### Response:\\n\" + data_point[\"response_cleaned\"]\n","\n","    return prompt\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:00.989468Z","iopub.execute_input":"2024-11-22T00:41:00.990257Z","iopub.status.idle":"2024-11-22T00:41:00.995455Z","shell.execute_reply.started":"2024-11-22T00:41:00.990222Z","shell.execute_reply":"2024-11-22T00:41:00.994390Z"},"id":"CsoGDrblajjj","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":12,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def tokenize(prompt, tokenizer, max_length=512, padding_type='max_length'):\n","    \"\"\"\n","    Tokenize the input text using specified tokenizer settings. This function prepares the text for\n","    processing by NLP models by converting the text into a sequence of IDs, considering maximum length\n","    and padding.\n","\n","    :param prompt: str, Input text to be tokenized\n","    :param tokenizer: Tokenizer object, the tokenizer to use for processing the text\n","    :param max_length: int, maximum sequence length for tokenization\n","    :param padding_type: str, type of padding to apply ('max_length' or 'longest')\n","    :return: dict, containing 'input_ids' and 'attention_mask', both truncated and padded as necessary\n","    \"\"\"\n","    # Ensure tokenizer has a padding token\n","    if tokenizer.pad_token is None:\n","        if tokenizer.eos_token is not None:\n","            tokenizer.pad_token = tokenizer.eos_token\n","        else:\n","            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","            # You need to resize model embeddings if new tokens are added\n","            model.resize_token_embeddings(len(tokenizer))\n","\n","    try:\n","        result = tokenizer(prompt, truncation=True, max_length=max_length + 1, padding=padding_type)\n","        return {\n","            'input_ids': result['input_ids'][:-1],\n","            'attention_mask': result['attention_mask'][:-1]\n","        }\n","    except Exception as e:\n","        print(f\"An error occurred during tokenization: {e}\")\n","        return {}\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:02.345537Z","iopub.execute_input":"2024-11-22T00:41:02.345968Z","iopub.status.idle":"2024-11-22T00:41:02.352478Z","shell.execute_reply.started":"2024-11-22T00:41:02.345935Z","shell.execute_reply":"2024-11-22T00:41:02.351537Z"},"id":"Ypsk-xrGajjk","executionInfo":{"status":"aborted","timestamp":1732237857698,"user_tz":300,"elapsed":12,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Example usage\n","prompt = \"Example prompt text to be tokenized.\"\n","tokenized_output = tokenize(prompt, tokenizer, max_length=512, padding_type='max_length')\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:03.707987Z","iopub.execute_input":"2024-11-22T00:41:03.708650Z","iopub.status.idle":"2024-11-22T00:41:03.713160Z","shell.execute_reply.started":"2024-11-22T00:41:03.708601Z","shell.execute_reply":"2024-11-22T00:41:03.712249Z"},"id":"fH9NTbaAajjk","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":12,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def generate_and_tokenize_prompt(data_point, tokenizer, max_length=512):\n","    \"\"\"Generate and tokenize a prompt with a masked response for training.\n","\n","    Args:\n","        data_point (dict): Contains 'instruction', optional 'context_cleaned', and 'response_cleaned'.\n","        tokenizer (Tokenizer): Tokenizer to use for tokenization.\n","        max_length (int): Maximum length of the tokenized input.\n","\n","    Returns:\n","        dict: Contains 'input_ids', 'labels' for loss calculation, and 'attention_mask'.\n","    \"\"\"\n","    # Generate the initial part of the prompt\n","    if data_point['context_cleaned']:\n","        user_prompt = (\n","            f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n","            f\"Write a response that appropriately completes the request.\\n\\n\"\n","            f\"### Instruction:\\n{data_point['instruction']}\\n\\n\"\n","            f\"### Input:\\n{data_point['context_cleaned']}\\n\\n\"\n","            f\"### Response:\\n\"\n","        )\n","    else:\n","        user_prompt = (\n","            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n","            f\"### Instruction:\\n{data_point['instruction']}\\n\\n\"\n","            f\"### Response:\\n\"\n","        )\n","\n","    # Tokenize the user prompt to determine the number of tokens\n","    prompt_tokens = tokenizer(user_prompt, add_special_tokens=False)\n","\n","    # Tokenize the full prompt including the response\n","    full_prompt = user_prompt + data_point['response_cleaned']\n","    full_tokens = tokenizer(full_prompt, max_length=max_length, truncation=True, padding='max_length')\n","\n","    # Calculate lengths and create masks\n","    len_user_prompt_tokens = len(prompt_tokens['input_ids'])\n","    labels = [-100] * len_user_prompt_tokens + full_tokens['input_ids'][len_user_prompt_tokens:]\n","\n","    return {\n","        'input_ids': full_tokens['input_ids'],\n","        'labels': labels,\n","        'attention_mask': full_tokens['attention_mask']\n","    }\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:07.866392Z","iopub.execute_input":"2024-11-22T00:41:07.867075Z","iopub.status.idle":"2024-11-22T00:41:07.873782Z","shell.execute_reply.started":"2024-11-22T00:41:07.867041Z","shell.execute_reply":"2024-11-22T00:41:07.872673Z"},"id":"oEBp4IXfajjk","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":11,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from datasets import DatasetDict\n","\n","def prepare_data(sampled_data_dict, val_set_size=0.1, generate_and_tokenize_func=None, random_seed=42):\n","    \"\"\"\n","    Prepare training and validation datasets by applying a tokenization function to each.\n","\n","    Args:\n","        sampled_data_dict (DatasetDict): The dataset dictionary containing the training data.\n","        val_set_size (float): The proportion of the dataset to be used as the validation set.\n","        generate_and_tokenize_func (callable): The function to apply to each data point for tokenization.\n","        random_seed (int): Seed for reproducibility of the dataset split.\n","\n","    Returns:\n","        tuple: A tuple containing the tokenized training and validation datasets. If no validation\n","               set is required (val_set_size <= 0), the second element in the tuple will be None.\n","    \"\"\"\n","    if val_set_size > 0:\n","        # Split the dataset into training and validation sets according to the specified proportion\n","        train_val_split = sampled_data_dict['train'].train_test_split(\n","            test_size=val_set_size,\n","            shuffle=True,  # Ensure the data is shuffled to prevent ordering biases affecting learning\n","            seed=random_seed  # Use the seed for reproducibility\n","        )\n","        # Apply the provided tokenization function to both the training and validation datasets\n","        train_data = train_val_split['train'].map(generate_and_tokenize_func, batched=True)\n","        val_data = train_val_split['test'].map(generate_and_tokenize_func, batched=True)\n","    else:\n","        # If no validation set is specified, apply the function to the entire training dataset\n","        train_data = sampled_data_dict['train'].map(generate_and_tokenize_func, batched=True)\n","        val_data = None  # Set the validation dataset to None\n","\n","    return train_data, val_data\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:10.498149Z","iopub.execute_input":"2024-11-22T00:41:10.498508Z","iopub.status.idle":"2024-11-22T00:41:10.504919Z","shell.execute_reply.started":"2024-11-22T00:41:10.498476Z","shell.execute_reply":"2024-11-22T00:41:10.504079Z"},"id":"oLVTcdf4ajjk","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":10,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from transformers import TrainerCallback\n","import math\n","\n","class PerplexityCallback(TrainerCallback):\n","    \"\"\"\n","    Custom callback to log and print perplexity at each logging step during training and validation.\n","    \"\"\"\n","    def __init__(self, loss_threshold=100):\n","        \"\"\"\n","        Initializes the callback with an optional loss threshold to handle large loss values gracefully.\n","\n","        Args:\n","            loss_threshold (float): Threshold above which loss is considered too large for stable exponentiation.\n","        \"\"\"\n","        self.loss_threshold = loss_threshold\n","\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        \"\"\"\n","        Event called at each logging step.\n","\n","        Args:\n","            args: Training arguments.\n","            state: TrainerState; provides information on training state.\n","            control: TrainerControl; provides various control flags.\n","            logs (dict): Dictionary of logs containing at least loss.\n","        \"\"\"\n","        if logs is not None and \"loss\" in logs:\n","            # Calculate perplexity from the training loss\n","            perplexity = math.exp(logs[\"loss\"]) if logs[\"loss\"] < self.loss_threshold else float(\"inf\")\n","            logs[\"perplexity\"] = perplexity\n","            # Optionally, you could add validation perplexity calculations here as well\n","            if \"eval_loss\" in logs:\n","                eval_perplexity = math.exp(logs[\"eval_loss\"]) if logs[\"eval_loss\"] < self.loss_threshold else float(\"inf\")\n","                logs[\"eval_perplexity\"] = eval_perplexity\n","\n","            # Log training and validation metrics\n","            print(f\"Step {state.global_step} - Training Loss: {logs['loss']:.4f} - Training Perplexity: {perplexity:.4f}\")\n","            if \"eval_loss\" in logs:\n","                print(f\"Step {state.global_step} - Validation Loss: {logs['eval_loss']:.4f} - Validation Perplexity: {eval_perplexity:.4f}\")\n","\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:12.847479Z","iopub.execute_input":"2024-11-22T00:41:12.847833Z","iopub.status.idle":"2024-11-22T00:41:12.858698Z","shell.execute_reply.started":"2024-11-22T00:41:12.847803Z","shell.execute_reply":"2024-11-22T00:41:12.857973Z"},"id":"myfjsCHsajjl","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":9,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the dataset from Hugging Face's dataset repository\n","dataset = load_dataset('kunchum/capstone_1')\n","\n","# Print information about the dataset to confirm it's loaded correctly\n","print(dataset)\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:15.200244Z","iopub.execute_input":"2024-11-22T00:41:15.200605Z","iopub.status.idle":"2024-11-22T00:41:15.849567Z","shell.execute_reply.started":"2024-11-22T00:41:15.200554Z","shell.execute_reply":"2024-11-22T00:41:15.848649Z"},"id":"MV_Y-rtJajjl","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":9,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["import torch\n","import sys\n","from datasets import load_dataset, DatasetDict\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","\n","# Load the tokenizer and model for Mistral AI\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n","model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n","\n","# Function to modify the model's state dictionary to incorporate PEFT\n","def apply_peft_to_model(model):\n","    old_state_dict = model.state_dict  # Store the original state_dict method\n","    model.state_dict = (\n","        lambda self, *args, **kwargs: get_peft_model_state_dict(self, old_state_dict(*args, **kwargs))\n","    ).__get__(model, type(model))  # Update the state_dict to use the PEFT version\n","    return model\n","\n","# Optionally compile the model with PyTorch 2.0 if applicable and not on Windows\n","if torch.__version__ >= '2' and sys.platform != 'win32':\n","    model = torch.compile(model)\n","\n","# Apply PEFT modifications\n","model = apply_peft_to_model(model)\n","\n","# Load and prepare your dataset\n","dataset = load_dataset('kunchum/capstone_1')\n","dataset = dataset.shuffle(seed=42)\n","\n","# Define a function for data preparation using the correct tokenizer\n","def tokenize_and_prepare(data_point):\n","    encoded = tokenizer(data_point['text'], padding=\"max_length\", truncation=True, max_length=512)\n","    return {'input_ids': encoded['input_ids'], 'attention_mask': encoded['attention_mask'], 'labels': encoded['input_ids']}\n","\n","# Prepare data\n","sampled_data_dict = DatasetDict({\n","    'train': dataset['train'].select(range(20000))\n","})\n","train_val_split = sampled_data_dict['train'].train_test_split(test_size=0.1)\n","train_data = train_val_split['train'].map(tokenize_and_prepare, batched=True)\n","val_data = train_val_split['test'].map(tokenize_and_prepare, batched=True)\n","\n","# Training arguments setup\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=50,\n","    save_strategy=\"steps\",\n","    save_steps=50,\n","    load_best_model_at_end=True,\n","    report_to=\"wandb\"\n",")\n","\n","# Setup the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=val_data,\n","    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",")\n","\n","\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T00:41:29.286768Z","iopub.execute_input":"2024-11-22T00:41:29.287108Z"},"id":"ecqr8COJajjl","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":8,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Start training\n","trainer.train()\n","\n","# Close the W&B run\n","wandb.finish()"],"metadata":{"trusted":true,"id":"tG4aoKqyajjl","executionInfo":{"status":"aborted","timestamp":1732237857699,"user_tz":300,"elapsed":8,"user":{"displayName":"Laasya Anantha Prasad","userId":"13966426256818682754"}}},"outputs":[],"execution_count":null}]}