{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"d61a3d26e97c4c809551942e1f618ba2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_342eb260a1634fc29bcb8ae17e5e9523","IPY_MODEL_31e706f4b135472c8d742399ffa1daef","IPY_MODEL_695902083e0549dd88e067f7d8693e44"],"layout":"IPY_MODEL_8b606c5adb8640f99bb9b994f6f28ce2"}},"342eb260a1634fc29bcb8ae17e5e9523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02e005ee1cab41b28662630a27404d65","placeholder":"​","style":"IPY_MODEL_eb412f0d60a64809b6df0616072d1bcf","value":"Map: 100%"}},"31e706f4b135472c8d742399ffa1daef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06154ad13886417489287e120300bd6c","max":500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29007c144ba04161b8449da8b2de2f36","value":500}},"695902083e0549dd88e067f7d8693e44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ddf2172c3d4aa3b7efdba16e2394c6","placeholder":"​","style":"IPY_MODEL_3fd1535ad87349819b4d229a5a01a99d","value":" 500/500 [00:01&lt;00:00, 354.23 examples/s]"}},"8b606c5adb8640f99bb9b994f6f28ce2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02e005ee1cab41b28662630a27404d65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb412f0d60a64809b6df0616072d1bcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06154ad13886417489287e120300bd6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29007c144ba04161b8449da8b2de2f36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0ddf2172c3d4aa3b7efdba16e2394c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fd1535ad87349819b4d229a5a01a99d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Fine-Tuning LLMs"],"metadata":{"id":"LnNOZOfqV-NJ"},"id":"LnNOZOfqV-NJ"},{"cell_type":"markdown","source":["In this exercise, you will fine-tune the [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model for enhanced dialogue summarization. You will first explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter-Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."],"metadata":{"id":"78bbda16"},"id":"78bbda16"},{"cell_type":"code","source":["''' gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info) '''"],"metadata":{"id":"ww4YXK5YOoWA"},"id":"ww4YXK5YOoWA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')'''"],"metadata":{"id":"JurhMQ_dOtmi"},"id":"JurhMQ_dOtmi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Set up Dependencies and Load Dataset and LLM"],"metadata":{"id":"ae38df4f"},"id":"ae38df4f"},{"cell_type":"code","source":["!pip install datasets evaluate rouge_score peft -q"],"metadata":{"id":"cfaa9a71","execution":{"iopub.status.busy":"2024-04-11T04:01:43.004061Z","iopub.execute_input":"2024-04-11T04:01:43.004404Z","iopub.status.idle":"2024-04-11T04:01:59.663674Z","shell.execute_reply.started":"2024-04-11T04:01:43.004377Z","shell.execute_reply":"2024-04-11T04:01:59.662384Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"cfaa9a71"},{"cell_type":"code","source":["import torch\n","import time\n","import evaluate\n","import pandas as pd\n","import numpy as np\n","\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from transformers import Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n","from datasets import load_dataset\n","from peft import LoraConfig, TaskType, get_peft_model"],"metadata":{"id":"7bfc5e61","execution":{"iopub.status.busy":"2024-04-11T04:01:59.666150Z","iopub.execute_input":"2024-04-11T04:01:59.666557Z","iopub.status.idle":"2024-04-11T04:02:15.696555Z","shell.execute_reply.started":"2024-04-11T04:01:59.666514Z","shell.execute_reply":"2024-04-11T04:02:15.695709Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"7bfc5e61"},{"cell_type":"code","source":["dataset = load_dataset('knkarthick/dialogsum')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e0022db","outputId":"99980fcc-50f7-4f0d-dbdc-9e9b6b2e41e8","execution":{"iopub.status.busy":"2024-04-11T04:02:15.699425Z","iopub.execute_input":"2024-04-11T04:02:15.700548Z","iopub.status.idle":"2024-04-11T04:02:19.702957Z","shell.execute_reply.started":"2024-04-11T04:02:15.700520Z","shell.execute_reply":"2024-04-11T04:02:19.702137Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"id":"9e0022db"},{"cell_type":"markdown","source":["Load the pre-trained [Flan-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of Flan-T5. Setting `torch_dtype=torch.bfloat16` specifies the data type to be used by this model, which can reduce GPU memory usage since `bfloat16` uses half as much memory per number compared to `float32`, the default precision for most models."],"metadata":{"id":"829a483e"},"id":"829a483e"},{"cell_type":"code","source":["model_name = 'google/flan-t5-base'\n","\n","original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"3b475615","execution":{"iopub.status.busy":"2024-04-11T04:02:19.704962Z","iopub.execute_input":"2024-04-11T04:02:19.705265Z","iopub.status.idle":"2024-04-11T04:02:27.426260Z","shell.execute_reply.started":"2024-04-11T04:02:19.705239Z","shell.execute_reply":"2024-04-11T04:02:27.425410Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"3b475615"},{"cell_type":"markdown","source":["## 2. Test the Model with Zero-Shot Inferencing\n","\n","Test the model with zero-shot inference."],"metadata":{"id":"404331c9"},"id":"404331c9"},{"cell_type":"code","source":["index = 42\n","dash_line = '-' * 100\n","\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n","inputs = tokenizer(prompt, return_tensors='pt')\n","output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n","original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n","\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{dialogue}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{original_model_summary}\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"903afec6","outputId":"a5800de1-981c-4e81-fd24-87ab308ef5d9","execution":{"iopub.status.busy":"2024-04-11T04:02:27.427541Z","iopub.execute_input":"2024-04-11T04:02:27.427862Z","iopub.status.idle":"2024-04-11T04:02:29.268801Z","shell.execute_reply.started":"2024-04-11T04:02:27.427835Z","shell.execute_reply":"2024-04-11T04:02:29.267772Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","----------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n","----------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","Person1: I'm not sure how to adjust my life.\n","\n"]}],"id":"903afec6"},{"cell_type":"markdown","source":["You can see that the model struggles to summarize the dialogue compared to the baseline summary, and simply repeats the first sentence from the dialogue."],"metadata":{"id":"hqKrbmjxvXhn"},"id":"hqKrbmjxvXhn"},{"cell_type":"markdown","source":["## 3. Perform Full Fine-Tuning"],"metadata":{"id":"39269c1c"},"id":"39269c1c"},{"cell_type":"markdown","source":["### 3.1 Preprocess the Dataset\n","\n","You need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation.`, and to the start of the summary with `Summary:` as follows:\n","\n","Training prompt (dialogue):\n","```\n","Summarize the following conversation.\n","Alice: This is her part of the conversation.\n","Bob: This is his part of the conversation.    \n","Summary:\n","```\n","\n","Training response (summary):\n","```\n","Both Alice and Bob participated in the conversation.\n","```"],"metadata":{"id":"ebb795b4"},"id":"ebb795b4"},{"cell_type":"markdown","source":["**Exercise**: Write a function to tokenize a batch of examples from the dialogue dataset. The function should concatentate the dialogues with the predefined prompt, tokenize them along with their summaries, and define the tokenized summaries as the labels."],"metadata":{"id":"EVGHecAnv8s0"},"id":"EVGHecAnv8s0"},{"cell_type":"code","source":["def tokenize(examples):\n","    # Create prompts by formatting the dialogue from the examples dataset into the desired input format\n","    prompts = [f\"Briefly summarize the conversation.\\n{conversation}\\nSummary:\" for conversation in examples[\"dialogue\"]]\n","\n","    # Extract the summary labels corresponding to each dialogue\n","    summaries = [summary_text for summary_text in examples[\"summary\"]]\n","\n","    # Tokenize the prompts using the tokenizer\n","    tokenized_prompts = tokenizer(prompts, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=512)\n","\n","    # Tokenize the summaries in a similar way, with a smaller max_length since summaries are shorter\n","    tokenized_summaries = tokenizer(summaries, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)\n","\n","    # Extract tokenized inputs for the prompts\n","    prompt_input_ids = tokenized_prompts.input_ids  # Encoded IDs for the tokenized text\n","    prompt_attention_mask = tokenized_prompts.attention_mask  # Attention mask for padded positions\n","\n","    # Extract tokenized inputs for the summaries (these serve as labels for the model)\n","    summary_input_ids = tokenized_summaries.input_ids  # Encoded IDs for the tokenized summaries\n","\n","    # Prepare the final dictionary to return\n","    model_inputs = {\n","        'input_ids': prompt_input_ids,\n","        'attention_mask': prompt_attention_mask,\n","        'labels': summary_input_ids\n","    }\n","\n","    # Return the processed inputs for model training or evaluation\n","    return model_inputs\n"],"metadata":{"id":"T6bMPv3ZK3UD","execution":{"iopub.status.busy":"2024-04-11T04:02:29.282607Z","iopub.execute_input":"2024-04-11T04:02:29.283360Z","iopub.status.idle":"2024-04-11T04:02:29.869938Z","shell.execute_reply.started":"2024-04-11T04:02:29.283326Z","shell.execute_reply":"2024-04-11T04:02:29.868667Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"T6bMPv3ZK3UD"},{"cell_type":"code","source":["tokenized_dataset = dataset.map(tokenize, batched=True)"],"metadata":{"id":"JPtVA3XzK5OG","execution":{"iopub.status.busy":"2024-04-11T04:02:29.871721Z","iopub.execute_input":"2024-04-11T04:02:29.872114Z","iopub.status.idle":"2024-04-11T04:02:41.769331Z","shell.execute_reply.started":"2024-04-11T04:02:29.872078Z","shell.execute_reply":"2024-04-11T04:02:41.768440Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["d61a3d26e97c4c809551942e1f618ba2","342eb260a1634fc29bcb8ae17e5e9523","31e706f4b135472c8d742399ffa1daef","695902083e0549dd88e067f7d8693e44","8b606c5adb8640f99bb9b994f6f28ce2","02e005ee1cab41b28662630a27404d65","eb412f0d60a64809b6df0616072d1bcf","06154ad13886417489287e120300bd6c","29007c144ba04161b8449da8b2de2f36","a0ddf2172c3d4aa3b7efdba16e2394c6","3fd1535ad87349819b4d229a5a01a99d"]},"outputId":"db198224-d65b-492c-9312-06211b890fdf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61a3d26e97c4c809551942e1f618ba2"}},"metadata":{}}],"id":"JPtVA3XzK5OG"},{"cell_type":"markdown","source":["### 3.2 Fine-Tune the Model\n","\n","**Exercise**: Utilize the Hugging Face Trainer API for training the model on the preprocessed dataset. Define the training arguments, a data collator, and create a `Seq2SeqTrainer` instance. Train the model for one epoch."],"metadata":{"id":"101bb5da"},"id":"101bb5da"},{"cell_type":"code","source":["# Define training arguments for the Seq2Seq model\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./output\",  # Directory to save all outputs, such as checkpoints and logs\n","    save_total_limit=2,  # Limit the number of checkpoints to save (to avoid using excessive disk space)\n","    learning_rate=5e-5,  # Set the learning rate for the optimizer\n","    per_device_train_batch_size=4,  # Batch size for training on each GPU/device\n","    per_device_eval_batch_size=4,  # Batch size for evaluation on each GPU/device\n","    gradient_accumulation_steps=4,  # Steps to accumulate gradients before updating parameters (effective batch size)\n","    optim=\"adamw_torch\",  # Optimizer to use for training; 'adamw_torch' is an efficient AdamW implementation\n","    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n","    num_train_epochs=1  # Number of epochs to train the model\n",")\n"],"metadata":{"id":"1ad8f449","execution":{"iopub.status.busy":"2024-04-11T04:07:20.796335Z","iopub.execute_input":"2024-04-11T04:07:20.796742Z","iopub.status.idle":"2024-04-11T04:07:20.807543Z","shell.execute_reply.started":"2024-04-11T04:07:20.796710Z","shell.execute_reply":"2024-04-11T04:07:20.806542Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"1ad8f449"},{"cell_type":"code","source":["# Define data collator\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)\n","\n","# Define the trainer for sequence-to-sequence training\n","trainer = Seq2SeqTrainer(\n","    model=original_model,  # The model to be trained (in this case, the original model)\n","    args=training_args,  # Training arguments (e.g., batch size, learning rate, number of epochs, etc.)\n","    data_collator=data_collator,  # Function to collate data into batches (handles padding and truncation)\n","    train_dataset=tokenized_dataset[\"train\"],  # The training dataset, preprocessed and tokenized\n","    eval_dataset=tokenized_dataset[\"validation\"],  # The evaluation/validation dataset, tokenized\n","    tokenizer=tokenizer,  # Tokenizer used for preprocessing input and decoding outputs\n",")"],"metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:07:21.682748Z","iopub.execute_input":"2024-04-11T04:07:21.683393Z","iopub.status.idle":"2024-04-11T04:07:21.702621Z","shell.execute_reply.started":"2024-04-11T04:07:21.683361Z","shell.execute_reply":"2024-04-11T04:07:21.701442Z"},"trusted":true,"id":"EQ0inh7iV-NU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"43cc0551-d933-46c8-9ae5-60d7052618b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"id":"EQ0inh7iV-NU"},{"cell_type":"markdown","source":["Training a fully fine-tuned version of the model should take about 10 minutes on a Google Colab GPU machine."],"metadata":{"id":"070b86e4"},"id":"070b86e4"},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"22e44303","outputId":"5666a7a4-d876-413b-9d29-28ce533b8f03","execution":{"iopub.status.busy":"2024-04-11T04:07:23.896463Z","iopub.execute_input":"2024-04-11T04:07:23.897389Z","iopub.status.idle":"2024-04-11T04:07:24.807399Z","shell.execute_reply.started":"2024-04-11T04:07:23.897353Z","shell.execute_reply":"2024-04-11T04:07:24.805571Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [59/59 39:35, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>23.023438</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=59, training_loss=26.831302966101696, metrics={'train_runtime': 2414.0846, 'train_samples_per_second': 5.161, 'train_steps_per_second': 0.024, 'total_flos': 8403342229241856.0, 'train_loss': 26.831302966101696, 'epoch': 0.98})"]},"metadata":{},"execution_count":12}],"id":"22e44303"},{"cell_type":"markdown","source":["Save the model to a local folder:"],"metadata":{"id":"bcOg9qQ_9M5M"},"id":"bcOg9qQ_9M5M"},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/Fall'24/LLM/Assignment 5/flan-t5-base_dialogsum\"\n","\n","original_model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"],"metadata":{"id":"klGQxAQf7prf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0b1761b-c3f7-4889-e5ef-996b0bf81975"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/Colab Notebooks/flan-t5-base-dialogsum-checkpoint/tokenizer_config.json',\n"," '/content/drive/MyDrive/Colab Notebooks/flan-t5-base-dialogsum-checkpoint/special_tokens_map.json',\n"," '/content/drive/MyDrive/Colab Notebooks/flan-t5-base-dialogsum-checkpoint/spiece.model',\n"," '/content/drive/MyDrive/Colab Notebooks/flan-t5-base-dialogsum-checkpoint/added_tokens.json',\n"," '/content/drive/MyDrive/Colab Notebooks/flan-t5-base-dialogsum-checkpoint/tokenizer.json')"]},"metadata":{},"execution_count":14}],"id":"klGQxAQf7prf"},{"cell_type":"markdown","source":["Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"],"metadata":{"id":"814042dd"},"id":"814042dd"},{"cell_type":"code","source":["# Load the fine-tuned instruction model for sequence-to-sequence tasks\n","instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n","    \"/content/drive/MyDrive/Fall'24/LLM/Assignment 5/flan-t5-base_dialogsum\",  # Path to the fine-tuned model\n","    torch_dtype=torch.bfloat32  # Use bfloat32 for reduced memory usage and faster computation on supported hardware\n",")\n"],"metadata":{"id":"d98f4126"},"execution_count":null,"outputs":[],"id":"d98f4126"},{"cell_type":"markdown","source":["Reload the original Flan-T5-base model:"],"metadata":{"id":"dK1EsTihBrHJ"},"id":"dK1EsTihBrHJ"},{"cell_type":"code","source":["original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)"],"metadata":{"id":"mdS7JS6PBpAh"},"execution_count":null,"outputs":[],"id":"mdS7JS6PBpAh"},{"cell_type":"markdown","source":["### 3.3 Evaluate the Model Qualitatively (Human Evaluation)\n","\n","**Exercise**: Make inferences for the same example as in Section 2, using the original model and the fully fine-tuned model."],"metadata":{"id":"964bc7e3"},"id":"964bc7e3"},{"cell_type":"code","source":["index = 42\n","dash_line = '-' * 100\n","\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n","inputs = tokenizer(prompt, return_tensors='pt')\n","\n","# Generate summary using the original model\n","original_model_output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n","original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n","\n","# Generate summary using the fine-tuned model\n","instruct_output = instruct_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n","instruct_summary = tokenizer.decode(instruct_output, skip_special_tokens=True)\n","\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{dialogue}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n","print(dash_line)\n","print(f'ORIGINAL MODEL GENERATION - ZERO SHOT:\\n{original_model_summary}\\n')\n","print(dash_line)\n","print(f'INSTRUCT MODEL GENERATION - ZERO SHOT:\\n{instruct_summary}\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e10df481","outputId":"acdc5eee-2a07-4910-d61b-178e366ddefb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","----------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n","----------------------------------------------------------------------------------------------------\n","ORIGINAL MODEL GENERATION - ZERO SHOT:\n","Person1: I'm not sure how to adjust my life.\n","\n","----------------------------------------------------------------------------------------------------\n","INSTRUCT MODEL GENERATION - ZERO SHOT:\n","#Person1#: I'm a little pale, and I can't sleep well every night. #Person2#: I'm a little pale, and I can't sleep well every night. #P\n","\n"]}],"id":"e10df481"},{"cell_type":"markdown","source":["The fine-tuned model is able to create a much better summary of the dialogue compared to the original model."],"metadata":{"id":"LBYIHoE9V-NW"},"id":"LBYIHoE9V-NW"},{"cell_type":"markdown","source":["### 3.4 Evaluate the Model Quantitatively (with ROUGE Metric)\n","\n","The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."],"metadata":{"id":"4b1ffbd2"},"id":"4b1ffbd2"},{"cell_type":"code","source":["rouge = evaluate.load('rouge')"],"metadata":{"id":"cf2a480b"},"execution_count":null,"outputs":[],"id":"cf2a480b"},{"cell_type":"markdown","source":["**Exercise**: Generate the outputs for a sample of the test set with the fine-tuned model (use only the first 10 dialogues and summaries to save time)."],"metadata":{"id":"in2SYXRfCNWA"},"id":"in2SYXRfCNWA"},{"cell_type":"code","source":["instruct_model_summaries = []  # Summaries generated by the fine-tuned model\n","original_model_summaries = []  # Summaries generated by the original model\n","human_baseline_summaries = []  # Human-written reference summaries\n","\n","def preprocess_and_generate(model, tokenizer, dialogue, device, max_length=512):\n","    \"\"\"\n","    Preprocess dialogue, generate a summary using the model, and decode the output.\n","    \"\"\"\n","    # Preprocess the dialogue\n","    tokenized_input = tokenizer(dialogue, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n","    # Generate the summary\n","    generated_output = model.generate(**tokenized_input)\n","    # Decode and return the summary\n","    return tokenizer.decode(generated_output[0], skip_special_tokens=True)\n","\n","\n","# Loop through the first 10 entries in the 'test' dataset\n","for i in range(10):\n","    # Extract the dialogue and the reference summary\n","    test_dialogue = dataset['test'][i]['dialogue']\n","    reference_summary = dataset['test'][i]['summary']\n","\n","    # Generate summaries using the instruct_model and original_model\n","    instruct_summary = preprocess_and_generate(instruct_model, tokenizer, test_dialogue, device)\n","    original_summary = preprocess_and_generate(original_model, tokenizer, test_dialogue, device)\n","\n","    # Append the summaries and reference to the respective lists\n","    instruct_model_summaries.append(instruct_summary)  # Fine-tuned model summary\n","    original_model_summaries.append(original_summary)  # Original model summary\n","    human_baseline_summaries.append(reference_summary)  # Human-written summary\n","\n"],"metadata":{"id":"pAOnB6lFCOUw"},"execution_count":null,"outputs":[],"id":"pAOnB6lFCOUw"},{"cell_type":"markdown","source":["Evaluate the models computing ROUGE metrics:"],"metadata":{"id":"4a019aaa"},"id":"4a019aaa"},{"cell_type":"code","source":["original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)]\n",")\n","\n","instruct_model_results = rouge.compute(\n","    predictions=instruct_model_summaries,\n","    references=human_baseline_summaries[0:len(instruct_model_summaries)]\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('INSTRUCT MODEL:')\n","print(instruct_model_results)"],"metadata":{"id":"d77847c0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3c5fdf5-f7a6-49d6-cda0-8beff8f35749"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ORIGINAL MODEL:\n","{'rouge1': 0.22868575868575866, 'rouge2': 0.08206617894882928, 'rougeL': 0.2006298146298146, 'rougeLsum': 0.20396599696599696}\n","INSTRUCT MODEL:\n","{'rouge1': 0.2494131054131054, 'rouge2': 0.09044485418029699, 'rougeL': 0.21345033893309756, 'rougeLsum': 0.21594925827684447}\n"]}],"id":"d77847c0"},{"cell_type":"markdown","source":["The results show substantial improvement in all ROUGE metrics:"],"metadata":{"id":"1c44701b"},"id":"1c44701b"},{"cell_type":"code","source":["print(\"Absolute percentage improvement of the instruct model over the original model:\")\n","\n","for key in instruct_model_results:\n","    improvement = instruct_model_results[key] - original_model_results[key]\n","    print(f'{key}: {improvement*100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dxy_WojXEAB","outputId":"773d483f-ded8-4528-fc78-fc32526fa40f"},"id":"6Dxy_WojXEAB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Absolute percentage improvement of the instruct model over the original model:\n","rouge1: 2.01%\n","rouge2: 0.89%\n","rougeL: 1.30%\n","rougeLsum: 1.29%\n"]}]},{"cell_type":"markdown","source":["## 4. Perform Parameter Efficient Fine-Tuning (PEFT)\n","\n","Now, let's perform **Parameter Efficient Fine-Tuning (PEFT)** instead of \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning, with comparable evaluation results as you will see soon.\n","\n","One of the most popular PEFT methods is **Low-Rank Adaptation (LoRA)**, which  introduces low-rank matrices to adapt the LLM with minimal additional parameters. In most cases, when someone says PEFT, they typically mean LoRA.  After fine-tuning for a specific task with LoRA, the result is that the original LLM remains unchanged and a newly-trained \"LoRA adapter\" emerges. This LoRA adapter is much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n","\n","At inference time, the LoRA adapter is reunited and combined with its original LLM to serve the inference request. The benefit is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."],"metadata":{"id":"Iq72DeUafKOL"},"id":"Iq72DeUafKOL"},{"cell_type":"markdown","source":["### 4.1 Setup the LoRA model for Fine-Tuning\n","\n","You first need to define the configuration of the LoRA model. Have a look at the configuration below. The key configuration element to adjust is the rank (`r`) of the adapter, which influences its capacity and complexity. Experiment with various ranks, such as 8, 16, or 32, and see how they affect the results."],"metadata":{"id":"jjMz_LZrfRKN"},"id":"jjMz_LZrfRKN"},{"cell_type":"code","source":["lora_config = LoraConfig(\n","    task_type=TaskType.SEQ_2_SEQ_LM,\n","    r=32,\n","    lora_alpha=32,\n","    lora_dropout=0.1\n",")"],"metadata":{"id":"c52eb8bf"},"execution_count":null,"outputs":[],"id":"c52eb8bf"},{"cell_type":"markdown","source":["Add LoRA adapter layers/parameters to the original LLM to be trained:"],"metadata":{"id":"lrxAQD2tflZA"},"id":"lrxAQD2tflZA"},{"cell_type":"code","source":["peft_model = get_peft_model(original_model, lora_config)"],"metadata":{"id":"1IHrKzPnfL-n"},"execution_count":null,"outputs":[],"id":"1IHrKzPnfL-n"},{"cell_type":"markdown","source":["The number of trainable model parameters in the LoRA model is:"],"metadata":{"id":"ta9nzcGgV-NZ"},"id":"ta9nzcGgV-NZ"},{"cell_type":"code","source":["peft_model.print_trainable_parameters()"],"metadata":{"id":"Zwv1srH-V-NZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e56ad1a8-4a2f-4d17-d33a-288c66a22ca2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 3,538,944 || all params: 251,116,800 || trainable%: 1.4092820552029972\n"]}],"id":"Zwv1srH-V-NZ"},{"cell_type":"markdown","source":["### 4.2 Train the LoRA Adapter\n","\n","**Exercise**: Define training arguments and create a `Seq2SeqTrainer` instance for the LoRA model. Use a higher learning rate than full fine-tuning (e.g., `1e-3`)."],"metadata":{"id":"zpLzokfYfo_m"},"id":"zpLzokfYfo_m"},{"cell_type":"code","source":["# Define training arguments with GPU device\n","peft_training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./output\",\n","    save_total_limit=2,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=4,\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=1,\n","    optim=\"adamw_torch\",\n","    learning_rate=1e-3\n",")\n"],"metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:07:20.796335Z","iopub.execute_input":"2024-04-11T04:07:20.796742Z","iopub.status.idle":"2024-04-11T04:07:20.807543Z","shell.execute_reply.started":"2024-04-11T04:07:20.796710Z","shell.execute_reply":"2024-04-11T04:07:20.806542Z"},"trusted":true,"id":"OA5gQz9O84zD"},"execution_count":null,"outputs":[],"id":"OA5gQz9O84zD"},{"cell_type":"code","source":["# Define data collator\n","#peft_data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)\n","# Define trainer\n","peft_trainer = Seq2SeqTrainer(\n","    model=original_model,\n","    args=peft_training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    )"],"metadata":{"execution":{"iopub.status.busy":"2024-04-11T04:07:21.682748Z","iopub.execute_input":"2024-04-11T04:07:21.683393Z","iopub.status.idle":"2024-04-11T04:07:21.702621Z","shell.execute_reply.started":"2024-04-11T04:07:21.683361Z","shell.execute_reply":"2024-04-11T04:07:21.701442Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"951431a4-2783-4a9f-9c32-4f8dd847bec8","id":"3ggjr_-D84zg"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"id":"3ggjr_-D84zg"},{"cell_type":"markdown","source":["Train the PEFT adapter. Training should take about 6 minutes on a Google Colab GPU machine."],"metadata":{"id":"H_2jryxZgMdR"},"id":"H_2jryxZgMdR"},{"cell_type":"code","source":["peft_trainer.train()"],"metadata":{"id":"f0d7T_P1gNlP","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"c9bc109e-a85f-4bf4-edb2-e3c5d4bba250"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [59/59 32:18, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>3.183594</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=59, training_loss=8.86670749470339, metrics={'train_runtime': 1970.8249, 'train_samples_per_second': 6.322, 'train_steps_per_second': 0.03, 'total_flos': 8536758945841152.0, 'train_loss': 8.86670749470339, 'epoch': 0.98})"]},"metadata":{},"execution_count":28}],"id":"f0d7T_P1gNlP"},{"cell_type":"markdown","source":["Save the model to a local folder:"],"metadata":{"id":"vJ_h6KU2gcUf"},"id":"vJ_h6KU2gcUf"},{"cell_type":"code","source":["peft_model.save_pretrained(\"/content/drive/MyDrive/Fall'24/LLM/Assignment 5/flan-t5-base_dialogsumlora\")"],"metadata":{"id":"7T9fwZ0NhOKC"},"execution_count":null,"outputs":[],"id":"7T9fwZ0NhOKC"},{"cell_type":"markdown","source":["Load the PEFT model:"],"metadata":{"id":"KmqvrFOrhVby"},"id":"KmqvrFOrhVby"},{"cell_type":"code","source":["peft_model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/Fall'24/LLM/Assignment 5/flan-t5-base_dialogsumlora\")\n","tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"],"metadata":{"id":"Wit_mz3Vgh-V"},"execution_count":null,"outputs":[],"id":"Wit_mz3Vgh-V"},{"cell_type":"markdown","source":["Reload the original Flan-T5-base model:"],"metadata":{"id":"dogDPmBBkuya"},"id":"dogDPmBBkuya"},{"cell_type":"code","source":["original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat32)"],"metadata":{"id":"EFur6v7nkItQ"},"execution_count":null,"outputs":[],"id":"EFur6v7nkItQ"},{"cell_type":"markdown","source":["### 4.3 Evaluate the Model Qualitatively (Human Evaluation)\n","\n","**Exercise**: Make inferences for the same example as in Sections 2 and 3, using the original model, the fully fine-tuned model and the PEFT model."],"metadata":{"id":"-AHhrS2lheOH"},"id":"-AHhrS2lheOH"},{"cell_type":"code","source":["index = 42\n","dash_line = '-' * 100\n","\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"Summarize the following conversation.\\n{dialogue}\\nSummary:\\n\"\n","inputs = tokenizer(prompt, return_tensors='pt')\n","\n","# Generate summary using the original model\n","original_model_output = original_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n","original_model_summary = tokenizer.decode(output, skip_special_tokens=True)\n","\n","# Generate summary using the fine-tuned model\n","peft_output = peft_model.generate(inputs['input_ids'], max_new_tokens=50)[0]\n","peft_summary = tokenizer.decode(peft_output, skip_special_tokens=True)\n","\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{dialogue}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n","print(dash_line)\n","print(f'ORIGINAL MODEL GENERATION - ZERO SHOT:\\n{original_model_summary}\\n')\n","print(dash_line)\n","print(f'PEFT MODEL GENERATION - ZERO SHOT:\\n{peft_summary}\\n')\n"],"metadata":{"id":"yGFEKSwAhXr6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4276ca6-c318-4e39-8028-3fa62fa959ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n","#Person2#: You look a bit pale, don't you?\n","#Person1#: Yes, I can't sleep well every night.\n","#Person2#: You should get plenty of sleep.\n","#Person1#: I drink a lot of wine.\n","#Person2#: If I were you, I wouldn't drink too much.\n","#Person1#: I often feel so tired.\n","#Person2#: You better do some exercise every morning.\n","#Person1#: I sometimes find the shadow of death in front of me.\n","#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n","----------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n","----------------------------------------------------------------------------------------------------\n","ORIGINAL MODEL GENERATION - ZERO SHOT:\n","Person1: I'm not sure how to adjust my life.\n","\n","----------------------------------------------------------------------------------------------------\n","PEFT MODEL GENERATION - ZERO SHOT:\n","#Person1# is worried about his future.\n","\n"]}],"id":"yGFEKSwAhXr6"},{"cell_type":"markdown","source":["### 4.4 Evaluate the Model Quantitatively (with ROUGE Metric)\n","\n","**Exercise**: Generate the outputs for a sample of the test set with the PEFT model (use only the first 10 dialogues and summaries to save time)."],"metadata":{"id":"YzNd3ptflWeS"},"id":"YzNd3ptflWeS"},{"cell_type":"code","source":["peft_model_summaries = []  # Summaries generated by the PEFT model\n","original_model_summaries = []  # Summaries generated by the original model\n","human_baseline_summaries = []  # Human-written reference summaries\n","\n","def preprocess_and_generate_summary(model, tokenizer, dialogue, device, max_length=512):\n","\n","    # Preprocess the dialogue\n","    tokenized_input = tokenizer(dialogue, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n","    # Generate the summary\n","    generated_output = model.generate(**tokenized_input)\n","    # Decode and return the summary\n","    return tokenizer.decode(generated_output[0], skip_special_tokens=True)\n","\n","# Loop through the first 10 entries in the 'test' dataset\n","for i in range(10):\n","    # Extract dialogue and reference summary\n","    test_dialogue = dataset['test'][i]['dialogue']\n","    reference_summary = dataset['test'][i]['summary']\n","\n","    # Generate summaries using the PEFT model and the original model\n","    peft_summary = preprocess_and_generate_summary(peft_model, tokenizer, test_dialogue, device)\n","    original_summary = preprocess_and_generate_summary(original_model, tokenizer, test_dialogue, device)\n","\n","    # Append the generated summaries and reference summary to respective lists\n","    peft_model_summaries.append(peft_summary)  # PEFT model summary\n","    original_model_summaries.append(original_summary)  # Original model summary\n","    human_baseline_summaries.append(reference_summary)  # Human-written summary\n"],"metadata":{"id":"lkDE0mtrkD1K"},"execution_count":null,"outputs":[],"id":"lkDE0mtrkD1K"},{"cell_type":"markdown","source":["Compute ROUGE score for this subset of the data."],"metadata":{"id":"TF6MuatKmFQz"},"id":"TF6MuatKmFQz"},{"cell_type":"code","source":["original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n",")\n","\n","instruct_model_results = rouge.compute(\n","    predictions=instruct_model_summaries,\n","    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",")\n","\n","peft_model_results = rouge.compute(\n","    predictions=peft_model_summaries,\n","    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('INSTRUCT MODEL:')\n","print(instruct_model_results)\n","print('PEFT MODEL:')\n","print(peft_model_results)"],"metadata":{"id":"bZmMVyDYmCDF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"241a1c98-51b4-41a1-b34b-fa4a80adc599"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ORIGINAL MODEL:\n","{'rouge1': 0.22868575868575866, 'rouge2': 0.08206617894882928, 'rougeL': 0.2006298146298146, 'rougeLsum': 0.20396599696599696}\n","INSTRUCT MODEL:\n","{'rouge1': 0.2494131054131054, 'rouge2': 0.09044485418029699, 'rougeL': 0.21345033893309756, 'rougeLsum': 0.21594925827684447}\n","PEFT MODEL:\n","{'rouge1': 0.2257931936299649, 'rouge2': 0.027705627705627706, 'rougeL': 0.16692994751073992, 'rougeLsum': 0.16711118615039638}\n"]}],"id":"bZmMVyDYmCDF"},{"cell_type":"markdown","source":["Notice, that PEFT model results are not too bad, while the training process was much easier!"],"metadata":{"id":"NViojzFXmRn1"},"id":"NViojzFXmRn1"},{"cell_type":"markdown","source":["Calculate the improvement of PEFT over the original model:"],"metadata":{"id":"C5CmTgHxmZBS"},"id":"C5CmTgHxmZBS"},{"cell_type":"code","source":["print(\"Absolute percentage improvement of the PEFT model over the original model:\")\n","\n","for key in peft_model_results:\n","    improvement = peft_model_results[key] - original_model_results[key]\n","    print(f'{key}: {improvement*100:.2f}%')"],"metadata":{"id":"X2JFTooimR_t","colab":{"base_uri":"https://localhost:8080/"},"outputId":"16321230-f377-448a-932e-6ef0d6e9beab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Absolute percentage improvement of the PEFT model over the original model:\n","rouge1: -0.29%\n","rouge2: -5.44%\n","rougeL: -3.37%\n","rougeLsum: -3.69%\n"]}],"id":"X2JFTooimR_t"},{"cell_type":"markdown","source":["Now calculate the improvement of PEFT over a full fine-tuned model:"],"metadata":{"id":"P2yCaFcWmbWh"},"id":"P2yCaFcWmbWh"},{"cell_type":"markdown","source":["#### Note : The code given does not use the abs() function so the percentage improvent is negative and it is not the absolute value"],"metadata":{"id":"oWumXOe5jst3"},"id":"oWumXOe5jst3"},{"cell_type":"code","source":["print(\"Absolute percentage improvement of the PEFT model over the instruct model:\")\n","\n","for key in peft_model_results:\n","    improvement = peft_model_results[key] - instruct_model_results[key]\n","    print(f'{key}: {improvement*100:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xr5-fRT3Owtq","outputId":"30250436-93f4-4c15-e91e-f666500a4248"},"id":"Xr5-fRT3Owtq","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Absolute percentage improvement of the PEFT model over the instruct model:\n","rouge1: -2.33%\n","rouge2: -6.24%\n","rougeL: -4.69%\n","rougeLsum: -4.99%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"AHLsXVtE_MVn"},"id":"AHLsXVtE_MVn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources."],"metadata":{"id":"Tpji7Bi6meQx"},"id":"Tpji7Bi6meQx"}]}